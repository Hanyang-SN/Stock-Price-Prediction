{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqmCnVryBKGj"
      },
      "source": [
        "# 1.데이터 다운로드 및 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD6-Tao0A8yl"
      },
      "source": [
        "## 1) pykrx 모듈 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu5TmDmuvTdv",
        "outputId": "384a97c8-680f-4e8d-c0ec-b70516c4af09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pykrx in /usr/local/lib/python3.10/dist-packages (1.0.45)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pykrx) (2.31.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pykrx) (1.5.3)\n",
            "Requirement already satisfied: datetime in /usr/local/lib/python3.10/dist-packages (from pykrx) (5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pykrx) (1.23.5)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from pykrx) (2.0.1)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from pykrx) (1.2.14)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from pykrx) (1.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from pykrx) (3.7.1)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.10/dist-packages (from datetime->pykrx) (6.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datetime->pykrx) (2023.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->pykrx) (1.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pykrx) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pykrx) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pykrx) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pykrx) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pykrx) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pykrx) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pykrx) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pykrx) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pykrx) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pykrx) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pykrx) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pykrx) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->pykrx) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime->pykrx) (67.7.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pykrx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF5t3JjIBHkE"
      },
      "source": [
        "## 2) 데이터 불러오기 (5년 치, 10년 치)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "do3cjAAoBet9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pykrx import stock\n",
        "\n",
        "# KOSDAQ 지수\n",
        "# df = stock.get_index_ohlcv_by_date(\"20100101\", \"20211231\", \"1001\")\n",
        "\n",
        "# KB금융\n",
        "df = stock.get_market_ohlcv(\"20180101\", \"20221231\", \"105560\")\n",
        "\n",
        "df_5years = df\n",
        "df_10years = stock.get_market_ohlcv(\"20130101\", \"20221231\", \"105560\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdQauhatFBXz"
      },
      "source": [
        "## 3) 데이터 정규화(Normalization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1wPj5xcEtSC",
        "outputId": "dcc69329-ccd5-4ca7-853f-b71117e8393a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1232, 6)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kylfk3K-Faw2"
      },
      "source": [
        "### (1) 5y, 5y-차분"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N23DyXN7THNq"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# graph 1\n",
        "series_5years = df_5years['종가']\n",
        "plt.title(\"5 years\")\n",
        "plt.plot(series_5years)\n",
        "plt.show()\n",
        "\n",
        "# graph 2\n",
        "plt.title(\"5 years, difference\")\n",
        "series_5years_diff = series_5years - series_5years.shift(1)\n",
        "plt.plot(series_5years_diff)\n",
        "plt.figure(figsize=(1,8))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZwcdCpjG4rl"
      },
      "source": [
        "### (2) 10y, 10y differencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ws8MuM4FrZc"
      },
      "outputs": [],
      "source": [
        "# graph 3\n",
        "plt.title(\"10 years\")\n",
        "series_10years = df_10years['종가']\n",
        "plt.plot(series_10years)\n",
        "plt.show()\n",
        "\n",
        "# graph 4\n",
        "plt.title(\"10 years, difference\")\n",
        "series_10years_diff = series_10years - series_10years.shift(1)\n",
        "plt.plot(series_10years_diff)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcke1VmwA8Aw"
      },
      "source": [
        "### (3) min-max 정규화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P7o60Mx4pog6"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "min_price = min(df['종가'])\n",
        "max_price = max(df['종가'])\n",
        "\n",
        "min_max_scaler = MinMaxScaler()\n",
        "df[\"종가\"] = min_max_scaler.fit_transform(df[\"종가\"].to_numpy().reshape(-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2w96sLtp1En"
      },
      "outputs": [],
      "source": [
        "df['종가']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzcBKutXIEww"
      },
      "source": [
        "# 2.Dataset 윈도우"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dIBH2MZMp3AB"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "class windowDataset(Dataset):\n",
        "  # data_stream     : input_window, output_window 크기에 따라 쪼개질 데이터\n",
        "  # input_window    : 인풋 기간\n",
        "  # output_window   : 아웃풋 기간\n",
        "  # stride          :\n",
        "    def __init__(self, data_stream, input_window=80, output_window=20, stride=5):\n",
        "        # data_stream의 행 개수를 구한다.\n",
        "        L = data_stream.shape[0]\n",
        "        # stride에 따라 샘플 개수를 구한다.\n",
        "        num_samples = (L - input_window - output_window) // stride + 1\n",
        "\n",
        "        # [window 크기 * sample 개수] 크기의, 0으로 채워진 배열을 만든다.\n",
        "        X = np.zeros([input_window, num_samples])\n",
        "        Y = np.zeros([output_window, num_samples])\n",
        "\n",
        "        # np.arange(num_samples): range(num_samples) 와 같음\n",
        "        for i in np.arange(num_samples):\n",
        "            # 1) X:   input_window 만큼 자르기 (stride * i ~)\n",
        "            start_x = stride * i\n",
        "            X[:,i] = data_stream[start_x:start_x + input_window]\n",
        "            # 2) Y:   output_window 만큼 자르기 (stride * i + input_window ~)\n",
        "            start_y = start_x + input_window\n",
        "            Y[:,i] = data_stream[start_y:start_y + output_window]\n",
        "\n",
        "\n",
        "        # shape       : [window 크기, sample 개수]\n",
        "        X = X.reshape(X.shape[0], X.shape[1], 1).transpose((1,0,2))\n",
        "        Y = Y.reshape(Y.shape[0], Y.shape[1], 1).transpose((1,0,2))\n",
        "        self.x = X\n",
        "        self.y = Y\n",
        "\n",
        "        self.len = len(X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.x[i], self.y[i]\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as3ta03WIkK2"
      },
      "source": [
        "# 3.Transformer 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "rPshqZFbp86W"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch import nn\n",
        "import torch\n",
        "import math\n",
        "\n",
        "class TFModel(nn.Module):\n",
        "\n",
        "# iw/ow:      input window, output window\n",
        "# d_model:    인풋 개수\n",
        "# nlayers:    인코더 부분의 인코더 개수\n",
        "# nhead:      multihead attention 개수\n",
        "\n",
        "    def __init__(self, iw: int, ow: int, d_model: int, nhead: int, nlayers: int, dropout=0.5):\n",
        "        super(TFModel, self).__init__()\n",
        "\n",
        "        # 1개 인코더, 인풋 사이즈가 d_model이고 attention 개수는 nhead\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
        "\n",
        "        # stacked 인코더, nlayers 만큼 쌓여있다.\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=nlayers)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        # 인풋 차원 변환. 1차원 -> d_model//2차워 -> d_model차원\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(1, d_model//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model//2, d_model)\n",
        "        )\n",
        "\n",
        "        # 차원 변환. d_model -> d_model//2 -> 1\n",
        "        self.linear =  nn.Sequential(\n",
        "            nn.Linear(d_model, d_model//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model//2, 1)\n",
        "        )\n",
        "\n",
        "        # 차원 변환. iw -> ow\n",
        "        self.linear2 = nn.Sequential(\n",
        "            nn.Linear(iw, (iw+ow)//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear((iw+ow)//2, ow)\n",
        "        )\n",
        "\n",
        "    def generate_square_subsequent_mask(self, size):\n",
        "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src, srcmask):\n",
        "        src = self.encoder(src)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src.transpose(0,1), srcmask).transpose(0,1)\n",
        "        output = self.linear(output)[:,:,0]\n",
        "        output = self.linear2(output)\n",
        "        return output\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "def gen_attention_mask(x):\n",
        "    mask = torch.eq(x, 0)\n",
        "    return mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEvoEQehIpqX"
      },
      "source": [
        "# 4.학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmQeCcBoN8fI"
      },
      "source": [
        "## 1) input, output 윈도우 사이즈 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Y74qg9doJU5X"
      },
      "outputs": [],
      "source": [
        "INPUT_WINDOW = 24*14\n",
        "OUTPUT_WINDOW = 24\n",
        "\n",
        "data_train = series_5years\n",
        "train_dataset = windowDataset(data_train, input_window=INPUT_WINDOW, output_window=OUTPUT_WINDOW, stride=1)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64)     # 64 = 2^6, 512 = 2^9\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "lr = 1e-4\n",
        "model = TFModel(iw=INPUT_WINDOW, ow=OUTPUT_WINDOW, d_model=512, nhead=8, nlayers=4, dropout=0.1).to(device)\n",
        "criterion = nn.MSELoss()                                            # MSEloss(): ow 각 요소들의 합\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL4fVi7KPAwG",
        "outputId": "59e4ec98-b880-43ab-e8a2-9224c853e575"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "날짜\n",
              "2018-01-02    63100\n",
              "2018-01-03    63100\n",
              "2018-01-04    63000\n",
              "2018-01-05    64100\n",
              "2018-01-08    66600\n",
              "Name: 종가, dtype: int32"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "_NnYlIderGyS",
        "outputId": "3eb3d86a-fa0c-4ad7-8a05-3b0322d986f7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "[error]\n",
        "OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 0; 14.75 GiB total capacity; 13.76 GiB already allocated; 70.81 MiB free; 13.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
        "\n",
        "[solution]\n",
        "https://discuss.pytorch.kr/t/cuda-out-of-memory/216/6\n",
        "'''\n",
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# for tqdm\n",
        "from tqdm import tqdm\n",
        "epoch = 30\n",
        "model.train()\n",
        "progress = tqdm(range(epoch))\n",
        "\n",
        "for i in progress:\n",
        "    batchloss = 0.0\n",
        "    for (inputs, outputs) in train_loader:\n",
        "      # inputs.shape: [batch_size, iw, 1]\n",
        "      # outputs.shape: [batch_size, ow, 1]\n",
        "\n",
        "      # Initialize grad\n",
        "      optimizer.zero_grad()                                           # zero_grad()로 Torch.Tensor.grad 초기화. 초기화하지 않으면 다음 루프 backward() 시에 간섭함.\n",
        "\n",
        "      # Forward propagation with masking\n",
        "      src_mask = model.generate_square_subsequent_mask(inputs.shape[1]).to(device)\n",
        "      result = model(inputs.float().to(device), src_mask)             # forward\n",
        "\n",
        "      # Backward propagation\n",
        "      loss = criterion(result, outputs[:,:,0].float().to(device))     # ?? 64개 중 하나만 loss를 담네?\n",
        "      loss.backward()                                                 # backward\n",
        "      optimizer.step()\n",
        "      batchloss += loss\n",
        "    progress.set_description(\"loss: {:0.6f}\".format(batchloss.cpu().item() / len(train_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iM3ACWWurGX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
