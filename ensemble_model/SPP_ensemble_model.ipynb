{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_D8afG71PBec"
      },
      "outputs": [],
      "source": [
        "!pip install pykrx\n",
        "!pip install sklearn\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onUbft1rRx-B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import unicodedata\n",
        "import math\n",
        "import torch, gc\n",
        "import torch.optim as optim\n",
        "from datetime import datetime, timedelta\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch import nn\n",
        "from pykrx import stock\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense, LSTM\n",
        "from keras.models import Sequential, load_model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rjmh7NJ_2Lj",
        "outputId": "0a4ee888-000d-4507-8438-9498f978f445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "구글 드라이브에 마운트\n",
        "드라이브에 저장된 데이터를 불러옴 (로컬에서 추가할 수도 있지만 런타임 해제되면 없어짐..ㅠㅠ)"
      ],
      "metadata": {
        "id": "m3vHyuE_uLDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "data폴더 내에 news_data, technical_data폴더가 위치"
      ],
      "metadata": {
        "id": "VcvjYQPrvHYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "폴더 경로 : /content/gdrive/MyDrive/data"
      ],
      "metadata": {
        "id": "larsKH1jy7Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## \"KB금융\t105560 신한지주\t055550 하나금융지주\t086790 메리츠금융지주\t138040 기업은행\t024110 미래에셋증권\t006800 NH투자증권\t005940 삼성증권\t016360\".split()\n"
      ],
      "metadata": {
        "id": "A9WSmtJdzoKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1e3wMBfNvN-"
      },
      "source": [
        "\n",
        "# 1. 데이터 다운로드 및 전처리\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF5t3JjIBHkE"
      },
      "source": [
        "## 2) 데이터 불러오기 (5년 치, 10년 치, 50년 치)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfa7zTwkWMV_"
      },
      "source": [
        "### (1) 8개 종목 선택\n",
        "\n",
        "KB금융\t105560 신한지주\t055550 하나금융지주\t086790 메리츠금융지주\t138040 기업은행\t024110 미래에셋증권\t006800 NH투자증권\t005940 삼성증권\t016360"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do3cjAAoBet9"
      },
      "outputs": [],
      "source": [
        "# Make code dictionary.\n",
        "finance_code_dict = dict()\n",
        "finance_code_list = \"KB금융\t105560 신한지주\t055550 하나금융지주\t086790 메리츠금융지주\t138040 기업은행\t024110 미래에셋증권\t006800 NH투자증권\t005940 삼성증권\t016360\".split()\n",
        "for i in range(8):\n",
        "  finance_code_dict[finance_code_list[2*i]] = finance_code_list[2*i + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhkKYsLCYtoh",
        "outputId": "e0f720d3-35dd-4861-a394-b27984756aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'KB금융': '105560', '신한지주': '055550', '하나금융지주': '086790', '메리츠금융지주': '138040', '기업은행': '024110', '미래에셋증권': '006800', 'NH투자증권': '005940', '삼성증권': '016360'}\n"
          ]
        }
      ],
      "source": [
        "print(finance_code_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NME5ULkFacpt"
      },
      "source": [
        "{'KB금융': '105560', '신한지주': '055550', '하나금융지주': '086790', '메리츠금융지주': '138040', '기업은행': '024110', '미래에셋증권': '006800', 'NH투자증권': '005940', '삼성증권': '016360'}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxCbVrCeYeD-"
      },
      "source": [
        "### (2) 데이터 가져오기 함수 정의 (5y, 10y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn_sJpl5ePvU"
      },
      "outputs": [],
      "source": [
        "def get_5y_10y(ticker_name):\n",
        "  ticker_code = finance_code_dict[ticker_name]\n",
        "  return stock.get_market_ohlcv(\"20180101\", \"20221231\", ticker_code), stock.get_market_ohlcv(\"20130101\", \"20221231\", ticker_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG48bZ0AevsG"
      },
      "source": [
        "### (3) 데이터 그리기 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT7EkiKLey8m"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def draw_graph_10y(ticker_name):\n",
        "\n",
        "  _, df = get_5y_10y(ticker_name)\n",
        "\n",
        "  # 1 line, 3 graphs\n",
        "\n",
        "  # graph 1\n",
        "  plt.subplot(3, 1, 1)\n",
        "  series = df['종가']\n",
        "  plt.title(f\"{ticker_name} time series\")\n",
        "  plt.spring()\n",
        "  plt.plot(series)\n",
        "\n",
        "  # graph 2\n",
        "  plt.subplot(3, 1, 2)\n",
        "  plt.title(f\"{ticker_name} difference, time series\")\n",
        "  series_diff = series - series.shift(1)\n",
        "  plt.plot(series_diff)\n",
        "\n",
        "  # graph 3\n",
        "  plt.subplot(3, 1, 3)\n",
        "  plt.title(f\"{ticker_name} difference, histogram\")\n",
        "  plt.hist(series_diff)\n",
        "\n",
        "  plt.tight_layout()\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvftx5hWpKtX"
      },
      "source": [
        "### (4) train_data, test_data 얻는 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UvcQYYf7X1f"
      },
      "outputs": [],
      "source": [
        "# 데이터 기간 설정하는 부분\n",
        "start_date_train = \"20210101\"\n",
        "end_date_train = \"20230101\"\n",
        "start_date_test = \"20230101\"\n",
        "end_date_test = \"20230630\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ekm6NeipKEy"
      },
      "outputs": [],
      "source": [
        "def get_10y_data(ticker_name): #train, test데이터 따로 df으로 -> 통합하여 리턴하도록 변경함\n",
        "  ticker_code = finance_code_dict[ticker_name]\n",
        "  selected_columns = ['종가']  # 포함하려는 열 이름 리스트\n",
        "  df = stock.get_market_ohlcv(start_date_train, end_date_test, ticker_code)\n",
        "  df = df.astype('float32')\n",
        "  return pd.DataFrame(df[selected_columns])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 주식 미개장일의 뉴스 데이터는 익일(개장날)에 반영 -> 미개장 기간 동안의 평균값을 반영해주는 함수."
      ],
      "metadata": {
        "id": "GBoRs-AuGbg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentimental_avg(code, name, df_news_data):\n",
        "  # 주식 미개장일의 뉴스 데이터는 익일(개장날)에 반영 -> 평균값을 반영하도록 구현하는 함수.\n",
        "  # 종가 데이터를 불러옴 -> train, test 나눠줘야함.\n",
        "  total_df = stock.get_market_ohlcv(start_date_train, end_date_test, code)\n",
        "  total_df = total_df['종가']\n",
        "  total_df = pd.DataFrame(total_df)\n",
        "  total_df['date'] = total_df.index\n",
        "\n",
        "  # 데이터프레임 A: 뉴스 데이터\n",
        "  df_a = df_news_data\n",
        "  df_a['Date'] = pd.to_datetime(df_a['date'])\n",
        "  df_a.set_index('Date', inplace=True)\n",
        "  print(df_a)\n",
        "\n",
        "  # 데이터프레임 B: 주가 데이터\n",
        "  price_data = total_df\n",
        "  df_b = pd.DataFrame(price_data)\n",
        "  df_b['Date'] = pd.to_datetime(df_b['date'])\n",
        "  df_b.set_index('Date', inplace=True)\n",
        "  print(df_b)\n",
        "\n",
        "  # 데이터프레임 A와 데이터프레임 B를 병합 (외부 조인)\n",
        "  merged_df = pd.merge(df_a, df_b, how='outer', left_index=True, right_index=True)\n",
        "\n",
        "  # NaN 값 (주말 뉴스)을 처리하여 다음 개장일의 뉴스 데이터에 반영\n",
        "  merged_df[name].fillna(method='ffill', inplace=True)\n",
        "\n",
        "  # 평균 계산하여 NaN 값을 대체\n",
        "  merged_df['종가'] = merged_df['종가'].fillna(0)  # NaN 값을 0으로 설정\n",
        "  next_open_day = merged_df.index[merged_df['종가'] != 0][0]  # 다음 개장일 찾기\n",
        "\n",
        "  # next_open_day=next_open_day+ timedelta(days=1)\n",
        "  avg_news_score = merged_df.loc[merged_df.index <= next_open_day, name].mean()\n",
        "  merged_df.loc[merged_df.index == next_open_day, name] = avg_news_score\n",
        "  merged_df = merged_df.drop(columns=['date_y','종가'])\n",
        "\n",
        "  return merged_df\n",
        "\n"
      ],
      "metadata": {
        "id": "8wWr1vNVPtVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzcBKutXIEww"
      },
      "source": [
        "# 2.Dataset 윈도우"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIBH2MZMp3AB"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "class windowDataset(Dataset):\n",
        "  # data_stream     : input_window, output_window 크기에 따라 쪼개질 데이터\n",
        "  # input_window    : 인풋 기간\n",
        "  # output_window   : 아웃풋 기간\n",
        "  # stride          :\n",
        "    def __init__(self, data_stream, input_window, output_window, n_features=3, stride=5):\n",
        "        # data_stream의 행 개수를 구한다.\n",
        "        L = data_stream.shape[0]\n",
        "        # stride에 따라 샘플 개수를 구한다.\n",
        "        num_samples = (L - input_window - output_window) // stride + 1\n",
        "\n",
        "        # [window 크기 * sample 개수] 크기의, 0으로 채워진 배열을 만든다.\n",
        "        X = np.zeros([input_window, num_samples, n_features])\n",
        "        Y = np.zeros([output_window, num_samples])\n",
        "\n",
        "        # np.arange(num_samples): range(num_samples) 와 같음\n",
        "        for i in np.arange(num_samples):\n",
        "            # 1) X:   input_window 만큼 자르기 (stride * i ~)\n",
        "            start_x = stride * i\n",
        "            X[:,i] = data_stream[start_x:start_x + input_window]\n",
        "            # 2) Y:   output_window 만큼 자르기 (stride * i + input_window ~)\n",
        "            start_y = start_x + input_window\n",
        "            Y[:,i] = data_stream[start_y:start_y + output_window]['종가']\n",
        "\n",
        "\n",
        "        # shape       : [window 크기, sample 개수]\n",
        "        X = X.reshape(X.shape[0], X.shape[1], n_features).transpose((1,0,2))\n",
        "        Y = Y.reshape(Y.shape[0], Y.shape[1], 1).transpose((1,0,2))\n",
        "        X=X.astype('float32')\n",
        "        Y=Y.astype('float32')\n",
        "        self.x = X\n",
        "        self.y = Y\n",
        "\n",
        "        self.len = len(X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.x[i], self.y[i]\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as3ta03WIkK2"
      },
      "source": [
        "# 3.Transformer 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPshqZFbp86W"
      },
      "outputs": [],
      "source": [
        "class TFModel(nn.Module):\n",
        "\n",
        "# iw/ow:      input window, output window\n",
        "# d_model:    인풋 개수\n",
        "# nlayers:    인코더 부분의 인코더 개수\n",
        "# nhead:      multihead attention 개수\n",
        "\n",
        "    def __init__(self, iw: int, ow: int, d_model: int, nhead: int, nlayers: int, dropout=0.5, n_features=3):\n",
        "        super(TFModel, self).__init__()\n",
        "\n",
        "        # TransformerEncoderLayer 인스턴스 생성 ) 1개 인코더, 인풋 사이즈가 d_model이고 attention 개수는 nhead\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
        "\n",
        "        # stacked 인코더, nlayers 만큼 쌓여있다.\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=nlayers)\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        # 인풋 차원 변환. 1차원 -> d_model//2차워 -> d_model차원\n",
        "        self.encoder = nn.Sequential(\n",
        "            # nn.Linear(1, d_model//2),\n",
        "            nn.Linear(n_features, d_model//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model//2, d_model)\n",
        "        )\n",
        "\n",
        "        # 차원 변환. d_model -> d_model//2 -> 1\n",
        "        self.linear =  nn.Sequential(\n",
        "            nn.Linear(d_model, d_model//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model//2, 1)\n",
        "        )\n",
        "\n",
        "        # 차원 변환. iw -> ow\n",
        "        self.linear2 = nn.Sequential(\n",
        "            nn.Linear(iw, (iw+ow)//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear((iw+ow)//2, ow)\n",
        "        )\n",
        "\n",
        "    def generate_square_subsequent_mask(self, size):\n",
        "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src, srcmask):\n",
        "        src = self.encoder(src)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src.transpose(0,1), srcmask).transpose(0,1)\n",
        "        output = self.linear(output)[:,:,0]\n",
        "        output = self.linear2(output)\n",
        "        return output\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "def gen_attention_mask(x):\n",
        "    mask = torch.eq(x, 0)\n",
        "    return mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEvoEQehIpqX"
      },
      "source": [
        "# 4.학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmQeCcBoN8fI"
      },
      "source": [
        "- 입출력 윈도우 사이즈\n",
        "- Learning Rate\n",
        "- Model\n",
        "  - layer\n",
        "  - dropout\n",
        "  - multihead attention 개수\n",
        "- Cost Function\n",
        "- Optimizer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVIsf8AzEPfO"
      },
      "source": [
        "# **lstm, svm 정의**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LSTM**"
      ],
      "metadata": {
        "id": "4uFphCAJTd0J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiFpLHJXxlKO"
      },
      "outputs": [],
      "source": [
        "def lstm_fit(): # lstm 데이터 생성 및 모델 구성, 학습하는 함수.\n",
        "\n",
        "  # 데이터 시퀀스\n",
        "  sequence_length = INPUT_WINDOW\n",
        "\n",
        "  lstm_X = []\n",
        "  lstm_y = []\n",
        "  lstm_test_X = []\n",
        "  lstm_test_y = []\n",
        "\n",
        "  for i in range(len(train_X) - sequence_length - 1):\n",
        "      lstm_X.append(train_X[i:i+sequence_length])\n",
        "      lstm_y.append([train_y[i+sequence_length]])\n",
        "\n",
        "  for i in range(len(test_X) - sequence_length - 1):\n",
        "      lstm_test_X.append(test_X[i:i+sequence_length])\n",
        "      lstm_test_y.append([test_y[i+sequence_length]])\n",
        "\n",
        "  lstm_X = np.array(lstm_X)\n",
        "  lstm_y = np.array(lstm_y)\n",
        "  lstm_test_X = np.array(lstm_test_X)\n",
        "  lstm_test_y = np.array(lstm_test_y)\n",
        "\n",
        "  # LSTM 모델 구성\n",
        "  lstm_model = Sequential()\n",
        "  lstm_model.add(LSTM(50, input_shape=(lstm_X.shape[1], lstm_X.shape[2]))) # 21,3\n",
        "  lstm_model.add(Dense(1))\n",
        "  lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "  # 모델 학습\n",
        "  lstm_model.fit(lstm_X, lstm_y, epochs=100, batch_size=32)\n",
        "\n",
        "  # 모델 실행\n",
        "  loss = lstm_model.evaluate(lstm_test_X, lstm_test_y)\n",
        "  print(f'Test Loss: {loss}')\n",
        "\n",
        "  return lstm_model, lstm_test_X, lstm_test_y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SVM**"
      ],
      "metadata": {
        "id": "CZ3LHNrMDzq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "svm 모델 생성하고 학습하는 함수\n"
      ],
      "metadata": {
        "id": "8D2i7M6CKPhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "linear kernel을 사용할 때 정확도가 소폭 상승함.\n",
        "\n",
        "*   항목 추가\n",
        "*   항목 추가\n",
        "\n"
      ],
      "metadata": {
        "id": "WCUwTxBSZKuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**분류 문제를 위한 상승, 하락 이진분류 라벨을 담은 배열을 만들어주는 함수.**"
      ],
      "metadata": {
        "id": "CbK-nvT2mQ6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_binary_classify(train_data_y):\n",
        "  # 전일 대비 상승(1) 또는 하락(0) 여부를 담을 배열 초기화\n",
        "  price_movement = np.zeros(len(train_data_y), dtype=int)\n",
        "\n",
        "  # 주가 데이터를 기반으로 전일 대비 상승 또는 하락 여부를 계산\n",
        "  for i in range(1, len(train_data_y)):\n",
        "      if train_data_y[i] > train_data_y[i - 1]:\n",
        "          price_movement[i] = 1  # 상승\n",
        "      elif train_data_y[i] < train_data_y[i - 1]:\n",
        "          price_movement[i] = 0  # 하락\n",
        "\n",
        "  return price_movement"
      ],
      "metadata": {
        "id": "H6VNi7hHeCEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_fit():\n",
        "  train_y_binary = svm_binary_classify(train_y)\n",
        "  test_y_binary = svm_binary_classify(test_y)\n",
        "\n",
        "  #  SVM 모델 생성 및 학습\n",
        "  svm_model = SVC(kernel='linear', C=1)\n",
        "  svm_model.fit(train_X, train_y_binary)\n",
        "\n",
        "  return svm_model, test_y_binary\n"
      ],
      "metadata": {
        "id": "2kqjiivcidYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LSTM, SVM 예측 및 정확도 측정**"
      ],
      "metadata": {
        "id": "S0UeQ2hGHiOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**lstm 정확도 측정 함수**"
      ],
      "metadata": {
        "id": "IMBjWN_4IDxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(predicted_prices, actual_prices):\n",
        "    if len(actual_prices) != len(predicted_prices):\n",
        "        print(len(actual_prices),len(predicted_prices))\n",
        "        raise ValueError(\"데이터의 길이가 일치해야 합니다.\")\n",
        "\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for i in range(len(actual_prices) - 1):\n",
        "        if (predicted_prices[i+1] - actual_prices[i]) * (actual_prices[i+1] - actual_prices[i]) >= 0:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    accuracy = (correct_predictions / (len(actual_prices) - 1)) * 100  # 정확도 계산 (마지막 데이터는 다음 데이터가 없어서 제외)\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "UQzKMSrKIC70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **✅ 실험 부분**"
      ],
      "metadata": {
        "id": "nzMSkF7uHAc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data setting**"
      ],
      "metadata": {
        "id": "Lyr-7LdbWPG6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Epdh1jiw8KY"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import zscore\n",
        "\n",
        "# 선택한 열의 값들을 z-score 정규화 적용하여 return 하는 함수\n",
        "\n",
        "def to_z_score(_data):\n",
        "  # selected_column = ['종가']\n",
        "  # selected_column_values = _data[selected_column]\n",
        "  numeric_cols = _data.select_dtypes(include='number').columns\n",
        "  _data[numeric_cols] = _data[numeric_cols].apply(zscore)\n",
        "  return _data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래 코드로 돌리려 했는데.. 구글 드라이브에서 파일 불러오는 중 오류 발생 ㅠㅠ(error num 107) => 파일 업로드 코드로 대체 실행"
      ],
      "metadata": {
        "id": "IFqVgaa1OAMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **구글드라이브에서 파일 불러오기**"
      ],
      "metadata": {
        "id": "clNXIgy8JJff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for key, value in finance_code_dict.items():\n",
        "#     key2=key.encode('utf-8').decode('utf-8')\n",
        "#     key=unicodedata.normalize('NFC', key2)\n",
        "#     # 뉴스 파일 불러옴\n",
        "#     for filename in files_news:\n",
        "#         filename=filename.encode('utf-8').decode('utf-8')\n",
        "#         filename=unicodedata.normalize('NFC',filename)\n",
        "#         if filename.startswith(key):\n",
        "#             file_path = os.path.join(directory_path_news, filename)\n",
        "#             try:\n",
        "#                 df_news_data = pd.read_csv(file_path)\n",
        "#             except FileNotFoundError:\n",
        "#                 print(f\"File '{filename}' not found.\")\n",
        "#             except Exception as e:\n",
        "#                 print(f\"An error occurred while opening '{filename}': {str(e)}\")\n",
        "#     # 기술지표 파일 불러옴\n",
        "#     for filename in files_tech:\n",
        "#         filename=filename.encode('utf-8').decode('utf-8')\n",
        "#         filename=unicodedata.normalize('NFC',filename)\n",
        "#         if filename.startswith(key):\n",
        "#             file_path = os.path.join(directory_path_tech, filename)\n",
        "#             try:\n",
        "#                 df_tech_data = pd.read_csv(file_path)\n",
        "#             except FileNotFoundError:\n",
        "#                 print(f\"File '{filename}' not found.\")\n",
        "#             except Exception as e:\n",
        "#                 print(f\"An error occurred while opening '{filename}': {str(e)}\")\n",
        "\n",
        "# print(df_news_data)\n",
        "# print(df_tech_data)\n"
      ],
      "metadata": {
        "id": "Wt--QB7t0GGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **파일 업로드**"
      ],
      "metadata": {
        "id": "VUP2u3vdHymb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 디렉토리 경로 설정\n",
        "directory_path_news = '/content/Data/news_data'\n",
        "directory_path_tech = '/content/Data/technical_data'\n",
        "\n",
        "# 디렉토리 내의 파일 목록 가져오기\n",
        "files_news = os.listdir(directory_path_news)\n",
        "print(files_news)\n",
        "files_tech = os.listdir(directory_path_tech)\n",
        "\n",
        "# # 디렉토리 경로 설정\n",
        "# directory_path_news = '/content/gdrive/MyDrive/data/news_data'\n",
        "# directory_path_tech = '/content/gdrive/MyDrive/data/technical_data'\n",
        "\n",
        "# # 디렉토리 내의 파일 목록 가져오기\n",
        "# files_news = os.listdir(directory_path_news)\n",
        "# files_tech = os.listdir(directory_path_tech)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17s1enEO020j",
        "outputId": "4b87f5c2-28d7-4380-fcb7-92e40f2a8b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['하나금융지주_20230701.csv', '미래에셋증권_20210101_to_20230630.csv', '기업은행_20230701.csv', '메리츠금융지주_20230701.csv', '삼성증권_20210101_to_20230630.csv', 'KB금융_20230701.csv', 'NH투자증권_20210101_to_20230630.csv', '신한지주_20230701.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **모델 실행 함수**"
      ],
      "metadata": {
        "id": "c5we720sPa4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **데이터 결합하여 리턴하는 함수 -> 일단은 안씀**"
      ],
      "metadata": {
        "id": "H4iRybQ-XA9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 기술지표 파일과 감성분석 파일을 종가 데이터와 결합하여 리턴하는 함수.\n",
        "def return_data(df_news_data, df_tech_data, stock_name, stock_code):\n",
        "    # 종가 데이터를 불러옴\n",
        "    price_data = get_10y_data(stock_name)\n",
        "    price_data['date'] = price_data.index\n",
        "    # print(price_data)\n",
        "\n",
        "    # 감성분석 파일 - 미 개장일 누적 데이터의 평균을 익일(개장일)에 반영하도록 하는 함수.\n",
        "    df_news_data = calculate_sentimental_avg(stock_code, stock_name, df_news_data)\n",
        "    df_news_data = df_news_data.rename(columns={'date_x': 'date'})\n",
        "    df_news_data = df_news_data.rename(columns={stock_name: 'news_score'})\n",
        "    # print(df_news_data)\n",
        "\n",
        "    # tech data 열 이름 변경 (날짜를 date로)\n",
        "    df_tech_data = df_tech_data.rename(columns={'날짜': 'date'})\n",
        "\n",
        "    # 종가 + 감성분석 + 기술지표 통합\n",
        "    price_data['date'] = pd.to_datetime(price_data['date'])\n",
        "    df_news_data['date'] = pd.to_datetime(df_news_data['date'])\n",
        "    df_tech_data['date'] = pd.to_datetime(df_tech_data['date'])\n",
        "    print(\"ok\")\n",
        "    result_df = pd.merge(pd.merge(price_data, df_news_data, on='date', how='inner'), df_tech_data, on='date', how='inner')\n",
        "\n",
        "    # 기술지표 계산 이슈 반영\n",
        "    end_row = 35 # 35일치 데이터를 삭제해 줌(macd 계산 이슈)\n",
        "    result_df = result_df.drop(result_df.index[0:end_row])\n",
        "    result_df = result_df.set_index('date')\n",
        "\n",
        "    # z-정규화 적용\n",
        "    result_df = to_z_score(result_df)\n",
        "\n",
        "    # train_data, test_data로 나누어주기\n",
        "    # 특정 날짜를 기준으로 데이터프레임 분할\n",
        "    date_string = start_date_test\n",
        "    datetime_obj = datetime.strptime(date_string, \"%Y%m%d\")\n",
        "    formatted_date = datetime_obj.strftime(\"%Y-%m-%d\")\n",
        "    split_date = formatted_date\n",
        "    print(split_date)\n",
        "\n",
        "    train_data = result_df[result_df.index <= split_date]  # split_date 이하의 데이터\n",
        "    test_data = result_df[result_df.index > split_date]   # split_date 이후의 데이터\n",
        "    print(train_data)\n",
        "    return train_data, test_data\n"
      ],
      "metadata": {
        "id": "ZaVmMK19NIFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **종목별로 파일 읽어와서 실행**"
      ],
      "metadata": {
        "id": "Jz18cpTRXJQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_db = []\n",
        "test_db = []"
      ],
      "metadata": {
        "id": "xhs8CnFQev3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 아래 코드 설명(코드가 김)\n",
        "\n",
        "*   주식 목록을 불러와,\n",
        "*   각 주식에 해당하는 뉴스 파일과 기술지표 파일을 폴더에서 찾고, 읽어와 df를 만들어 준다.\n",
        "* 종가 데이터는 price_data, 뉴스 데이터는 df_news_data, 기술지표는 df_tech_data에 저장됨\n",
        "* 데이터 통합\n",
        "* z 정규화 진행\n",
        "* train_df, test_db에 저장한다."
      ],
      "metadata": {
        "id": "RTd1aHeNNt08"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_db, test_db에 8종목의 실험할 df가 담기게 됨."
      ],
      "metadata": {
        "id": "725jHA7JN6K_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in finance_code_dict.items():\n",
        "    key2=key.encode('utf-8').decode('utf-8')\n",
        "    key=unicodedata.normalize('NFC', key2)\n",
        "    # 뉴스 파일 불러와 df_news_data에 저장\n",
        "    for filename in files_news:\n",
        "        filename=filename.encode('utf-8').decode('utf-8')\n",
        "        filename=unicodedata.normalize('NFC',filename)\n",
        "        if filename.startswith(key):\n",
        "            file_path = os.path.join(directory_path_news, filename)\n",
        "            try:\n",
        "                df_news_data = pd.read_csv(file_path)\n",
        "            except FileNotFoundError:\n",
        "                print(f\"news : File '{filename}' not found.\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred while opening '{filename}': {str(e)}\")\n",
        "    # 기술 지표 파일 불러와 df_tech_data에 저장\n",
        "    for filename in files_tech:\n",
        "        filename=filename.encode('utf-8').decode('utf-8')\n",
        "        filename=unicodedata.normalize('NFC',filename)\n",
        "        if filename.startswith(key):\n",
        "            file_path = os.path.join(directory_path_tech, filename)\n",
        "            try:\n",
        "                df_tech_data = pd.read_csv(file_path)\n",
        "            except FileNotFoundError:\n",
        "                print(f\"tec : File '{filename}' not found.\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred while opening '{filename}': {str(e)}\")\n",
        "\n",
        "    # run함수 실행하여 모델 실행하기\n",
        "    # 종가 데이터를 불러옴\n",
        "    stock_name=key\n",
        "    stock_code=value\n",
        "    price_data = get_10y_data(stock_name)\n",
        "    price_data['date'] = price_data.index\n",
        "\n",
        "    # 감성분석 - 미 개장일 누적 데이터의 평균을 익일(개장일)에 반영하도록 하는 함수.\n",
        "    df_news_data = calculate_sentimental_avg(stock_code, stock_name, df_news_data)\n",
        "    df_news_data = df_news_data.rename(columns={'date_x': 'date'})\n",
        "    df_news_data = df_news_data.rename(columns={stock_name: 'news_score'})\n",
        "\n",
        "    # tech data 열 이름 변경 (날짜를 date로)\n",
        "    # date열 이름 맞추기\n",
        "    df_tech_data = df_tech_data.rename(columns={'날짜': 'date'})\n",
        "    price_data['date'] = pd.to_datetime(price_data['date'])\n",
        "    df_news_data['date'] = pd.to_datetime(df_news_data['date'])\n",
        "    df_tech_data['date'] = pd.to_datetime(df_tech_data['date'])\n",
        "\n",
        "    # 데이터프레임 합치는 부분!✅\n",
        "\n",
        "    #종가+감성+기술\n",
        "    result_df = pd.merge(pd.merge(price_data, df_news_data, on='date', how='inner'), df_tech_data, on='date', how='inner')\n",
        "    #종가+감성\n",
        "    # result_df = pd.merge(price_data, df_news_data, on='date', how='inner')\n",
        "    #종가만..\n",
        "    # result_df = price_data\n",
        "\n",
        "    # 기술지표 계산 이슈 반영\n",
        "    end_row = 35 # 35일치 데이터를 삭제해 줌(macd 계산 이슈로 35일치가 NAN임.)\n",
        "    result_df = result_df.drop(result_df.index[0:end_row])\n",
        "    result_df = result_df.set_index('date')\n",
        "\n",
        "    # z-정규화 적용\n",
        "    result_df = to_z_score(result_df)\n",
        "\n",
        "    # train_data, test_data로 나누어주기\n",
        "    # 특정 날짜를 기준으로 데이터프레임 분할시킨다.\n",
        "    date_string = start_date_test\n",
        "    datetime_obj = datetime.strptime(date_string, \"%Y%m%d\")\n",
        "    formatted_date = datetime_obj.strftime(\"%Y-%m-%d\")\n",
        "    split_date = formatted_date\n",
        "\n",
        "    train_data = result_df[result_df.index <= split_date]  # split_date 이하의 데이터\n",
        "    test_data = result_df[result_df.index > split_date]   # split_date 이후의 데이터\n",
        "    train_db.append(train_data)\n",
        "    test_db.append(test_data)\n",
        "\n",
        "    # train_data, test_data = run(df_news_data, df_tech_data, key, value)\n",
        "\n"
      ],
      "metadata": {
        "id": "MiBlphbmNsdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **종목 선택**"
      ],
      "metadata": {
        "id": "6RwJE4ICVdvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index=1\n",
        "train_data = train_db[index]\n",
        "test_data = test_db[index]"
      ],
      "metadata": {
        "id": "2RndKVMJe_7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data)"
      ],
      "metadata": {
        "id": "HpnjuWhxkYIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "N_FEATURES = train_data.shape[1]\n"
      ],
      "metadata": {
        "id": "cOkhqGA-J6TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_FEATURES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbTGKeTyitSy",
        "outputId": "56710771-6edd-40dd-bfbc-a80a1a284227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 330
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **✅모델 부분**"
      ],
      "metadata": {
        "id": "DaZkwUFmOE1D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJgYL2S_XSJu"
      },
      "source": [
        "# TF model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ir = 1e-04\n",
        "batch size = 32"
      ],
      "metadata": {
        "id": "2mj-qpR7XOEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "7Y3JYGJ6LNWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y74qg9doJU5X"
      },
      "outputs": [],
      "source": [
        "# @title Hyper-parameter\n",
        "INPUT_WINDOW = 14\n",
        "OUTPUT_WINDOW = 14\n",
        "\n",
        "BATCH_SIZE= 32\n",
        "lr = 1e-4 # 학습률을 적당히 설정하는 게 중요함.\n",
        "\n",
        "train_dataset = windowDataset(train_data, input_window=INPUT_WINDOW, output_window=OUTPUT_WINDOW, stride=1, n_features=N_FEATURES)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)     # 64 = 2^6, 512 = 2^9\n",
        "test_dataset = windowDataset(test_data, input_window=INPUT_WINDOW, output_window=OUTPUT_WINDOW, stride=1, n_features=N_FEATURES)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)     # 64 = 2^6, 512 = 2^9\n",
        "\n",
        "tf_model = TFModel(iw=INPUT_WINDOW, ow=OUTPUT_WINDOW, d_model=512, nhead=8, nlayers=4, dropout=0.1, n_features=N_FEATURES).to(device)\n",
        "criterion = nn.MSELoss()                                            # MSEloss(): ow 각 요소들의 합\n",
        "optimizer = torch.optim.Adam(tf_model.parameters(), lr=lr)\n",
        "\n",
        "train_losses = []  # 각 에포크의 훈련 손실 값을 저장할 리스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NnYlIderGyS",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title TF Train\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# for tqdm\n",
        "from tqdm import tqdm\n",
        "\n",
        "# for trainig mode\n",
        "epoch = 50\n",
        "tf_model.train()\n",
        "progress = tqdm(range(epoch))\n",
        "\n",
        "# for drawing loss per epoch.\n",
        "max_non_improvement = 10  # 일정 기간동안 개선되지 않을 때 학습을 종료하기 위한 조건\n",
        "best_loss = float('inf')  # 최적의 손실 값을 추적하기 위한 변수\n",
        "no_improvement_count = 0  # 개선되지 않은 에포크 카운트\n",
        "\n",
        "for i in progress:\n",
        "  batchloss = 0.0\n",
        "  for (inputs, outputs) in train_loader:\n",
        "    # inputs.shape: [batch_size, iw, 1] -> 1 말구 num_of features\n",
        "    # outputs.shape: [batch_size, ow, 1]\n",
        "    # Initialize grad\n",
        "    optimizer.zero_grad()                                           # zero_grad()로 Torch.Tensor.grad 초기화. 초기화하지 않으면 다음 루프 backward() 시에 간섭함.\n",
        "    # 모델에 사용할 마스크 생성\n",
        "    # Forward propagation with masking\n",
        "    src_mask = tf_model.generate_square_subsequent_mask(inputs.shape[1]).to(device)\n",
        "\n",
        "    result = tf_model(inputs.float().to(device), src_mask)             # forward\n",
        "\n",
        "    # Backward propagation\n",
        "    loss = criterion(result, outputs[:,:,0].float().to(device))     # ?? 64개 중 하나만 loss를 담네?\n",
        "    # print(f\"[result]\\n{result}\\n\\n[output[:,:,0]]\\n{outputs[:,:,0]}\\n\\n[outputs]\\n{outputs}\")\n",
        "    loss.backward()                                                 # backward\n",
        "    optimizer.step()\n",
        "    batchloss += loss\n",
        "\n",
        "  print()\n",
        "  progress.set_description(f\"loss: {batchloss.cpu().item() / len(train_loader):0.6f}\")\n",
        "\n",
        "  # 훈련 손실 값 저장\n",
        "  train_losses.append(batchloss.cpu().item() / len(train_loader))\n",
        "\n",
        "  # 조기 종료 검사 및 학습 곡선 그리기\n",
        "  if batchloss < best_loss:\n",
        "    best_loss = batchloss\n",
        "    no_improvement_count = 0\n",
        "  else:\n",
        "    no_improvement_count += 1\n",
        "\n",
        "  if no_improvement_count >= max_non_improvement:\n",
        "    print(f\"Early stopping due to no improvement for {max_non_improvement} epochs.\")\n",
        "    break\n",
        "\n",
        "progress.close()\n",
        "\n",
        "# 학습 곡선 그리기\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIU5cu4p-LbU"
      },
      "source": [
        "## 3) TF Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iM3ACWWurGX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b64d44dd-5806-4680-f2e7-e5f701ac2e4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 3/3 [00:00<00:00, 126.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF model) Directional Accuracy: 44.791667%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# set evaluation mode\n",
        "tf_model.eval()\n",
        "\n",
        "# Initialize correct & total\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "#트렌스포머 모델의 이진분류 결과를 저장해주는 배열\n",
        "transformer_result_binary = []\n",
        "\n",
        "# 기울기 계산을 방지하기 위해 torch.no_grad() 블록 안에서 평가\n",
        "with torch.no_grad():\n",
        "  for (inputs, outputs) in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "    # Forward propagation with masking\n",
        "    src_mask = tf_model.generate_square_subsequent_mask(inputs.shape[1]).to(device)\n",
        "    result = tf_model(inputs.float().to(device), src_mask)\n",
        "\n",
        "\n",
        "    # 상승/하강 예측\n",
        "    predicted_changes = torch.sign(result[:, -1] - inputs[:, -1, 0].to(device))             # 마지막 예측 값 - 마지막 입력 값\n",
        "    true_changes = torch.sign(outputs[:, -1, 0].to(device) - inputs[:, -1, 0].to(device))  # 실제 마지막 값 - 마지막 입력 값\n",
        "\n",
        "    transformer_result_binary.append(true_changes.cpu()) #gpu에서 cpu로 데이터를 옮겨줘야 함..\n",
        "\n",
        "    # 예측이 맞는 경우\n",
        "    correct += (predicted_changes == true_changes).sum().item()\n",
        "    total += inputs.size(0)\n",
        "\n",
        "  progress.set_description(f\"current accuracy: {correct/total:0.6f}\")\n",
        "\n",
        "# 정확도 계산\n",
        "accuracy = correct / total\n",
        "print(f\"\\nTF model) Directional Accuracy: {accuracy * 100:.6f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM**"
      ],
      "metadata": {
        "id": "-6XDCP2xHsZc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxhMunZB7J43"
      },
      "outputs": [],
      "source": [
        "#lstm 데이터 세팅\n",
        "INPUT_WINDOW = 14\n",
        "OUTPUT_WINDOW = 14\n",
        "\n",
        "train_X = train_data.to_numpy()\n",
        "test_X = test_data.to_numpy()\n",
        "\n",
        "train_y = train_X[:,0]\n",
        "test_y = test_X[:,0]\n",
        "\n",
        "#트렌스포머 WINDOW사이즈를 고려하여 데이터를 맞춰준다.\n",
        "\n",
        "cut_size = INPUT_WINDOW - 1\n",
        "train_X=train_X[cut_size:,]\n",
        "train_y=train_y[cut_size:,]\n",
        "test_X=test_X[cut_size:,]\n",
        "test_y=test_y[cut_size:,]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하위 모델 생성\n",
        "lstm_model, lstm_test_X, lstm_test_y =  lstm_fit()  # LSTM 모델을 구성하고 학습한 모델\n",
        "\n",
        "# 하위 모델 예측\n",
        "lstm_predictions = lstm_model.predict(lstm_test_X)\n",
        "lstm_predictions = lstm_predictions.reshape(lstm_predictions.shape[0])\n",
        "lstm_test_y = lstm_test_y.reshape(lstm_test_y.shape[0])\n",
        "\n",
        "\n",
        "# lstm 예측 결과 시각화\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(lstm_predictions, label='Predicted')\n",
        "plt.plot(lstm_test_y, label='True')\n",
        "plt.title('lstm 예측 결과')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1DQOlIfvFttR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVM**"
      ],
      "metadata": {
        "id": "oC2tPnQIH_Yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 하위 모델 생성\n",
        "svm_model, test_y_binary = svm_fit()\n",
        "\n",
        "# 모델 예측\n",
        "svm_predictions = svm_model.predict(test_X)\n",
        "\n",
        "\n",
        "# 예측 결과 시각화(분류모델이라 의미가 없음..)\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# plt.scatter(test_y_binary, svm_predictions)\n",
        "# # plt.plot(x = test_y_binary, y=svm_predictions,kind='scatter')\n",
        "\n",
        "# # plt.plot(test_y_binary, label='True')\n",
        "# # plt.plot(svm_predictions, label='Predicted')\n",
        "# plt.title('svm 주가 예측 결과')\n",
        "# plt.legend()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "dlbPhOZgcrGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYSxAjNPS9up",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "183ee67d-0199-4f0f-f966-654be4aca4a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lstm 정확도: 64.893617%\n",
            "svm 정확도: 77.272727%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "lstm_accuracy = calculate_accuracy(lstm_predictions, lstm_test_y)\n",
        "\n",
        "svm_accuracy = accuracy_score(test_y_binary, svm_predictions)\n",
        "svm_accuracy = svm_accuracy * 100\n",
        "\n",
        "print(f\"lstm 정확도: {lstm_accuracy:.6f}%\")\n",
        "print(f\"svm 정확도: {svm_accuracy:.6f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hard Voting 진행**"
      ],
      "metadata": {
        "id": "vBwlkRit2n2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer_result_binary\n",
        "\n",
        "def replace_to_binary(arr):\n",
        "    result = np.where(arr > 0, 1, -1)\n",
        "    return result\n",
        "\n",
        "# 트렌스포머 결과 텐서 배열을 일반 1차원 배열으로 변경\n",
        "list_binary = [tensor.tolist() for tensor in transformer_result_binary]\n",
        "one_dimensional_array = [element for row in list_binary for element in row]\n",
        "transformer_predictions = one_dimensional_array\n",
        "\n",
        "svm_predictions = replace_to_binary(svm_predictions)\n",
        "lstm_predictions = replace_to_binary(lstm_predictions)\n",
        "\n",
        "svm_predictions = svm_predictions[INPUT_WINDOW - 1 : ]\n"
      ],
      "metadata": {
        "id": "LlZt1Ul5whQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_y_binary = replace_to_binary(test_y)\n",
        "test_y_binary = test_y_binary[INPUT_WINDOW - 1:]\n",
        "print(test_y_binary[:10])\n",
        "print(len(test_y_binary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLCh9UKBILwU",
        "outputId": "99da42fa-270b-44cb-abed-ada12a3dcb2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1  1  1  1  1 -1  1  1  1  1]\n",
            "97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다수결 실험 결과\n",
        "\n",
        "*   {tf, lstm, svm} = 앙상블 모델 정확도 : 58.823529%\n",
        "\n",
        "*   {lstm, svm} = 앙상블 모델 정확도 : 79.044118%\n",
        "\n",
        "\n",
        "*   {tf, lstm} = 앙상블 모델 정확도 : 73.774510%\n",
        "\n",
        "\n",
        "\n",
        "*   {tf, svm} = 앙상블 모델 정확도 : 52.450980%\n",
        "\n",
        "* 비고) 두 모델을 보팅할 경우, 두 모델의 예측값이 다를 때, (상승 + 하락) -> (상승)으로 예측 됨\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pWGgpcPlTaIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def majority_vote(predictions):\n",
        "    # 세 개의 예측 값을 비교하여 다수결로 의사결정\n",
        "    result = []\n",
        "    for i in range(len(predictions[0])):\n",
        "        votes = [predictions[j][i] for j in range(len(predictions))]\n",
        "        majority = max(set(votes), key=votes.count)\n",
        "        result.append(majority)\n",
        "    return result\n",
        "\n",
        "# 다수결로 의사결정\n",
        "combined_predictions = [ lstm_predictions, transformer_predictions[1:], svm_predictions[2:]]\n",
        "final_predictions = majority_vote(combined_predictions)\n"
      ],
      "metadata": {
        "id": "5aLRliS-4BLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy_for_voting(predictions, true_labels):\n",
        "    # 예측값과 실제값을 비교하여 정확도 계산\n",
        "    correct_count = sum(1 for p, t in zip(predictions, true_labels) if p == t)\n",
        "    total_count = len(true_labels)\n",
        "    accuracy = correct_count / total_count\n",
        "    return accuracy\n",
        "\n",
        "accuracy = accuracy_score(test_y_binary[2:], final_predictions)\n",
        "accuracy = accuracy * 100\n",
        "print(f'앙상블 모델 정확도 : {accuracy:.6f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6WEn6pN4C2T",
        "outputId": "b20cf08d-7d2d-4cde-be1f-778e1f88e2c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "앙상블 모델 정확도 : 70.526316%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 출력\n",
        "print(\"lstm\")\n",
        "print(lstm_predictions[:20])\n",
        "print(\"TF\")\n",
        "print(transformer_predictions[:20])\n",
        "print(\"svm\")\n",
        "print(svm_predictions[:20])\n",
        "print(\"다수결 예측 결과:\", final_predictions[:20])\n",
        "print(\"real val\")\n",
        "print(test_y_binary[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQbY6xNkI8V1",
        "outputId": "3730aa17-0e20-4e59-d88d-0a6e8716357c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lstm\n",
            "[ 1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
            "TF\n",
            "[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n",
            "svm\n",
            "[-1  1  1 -1 -1 -1 -1  1  1 -1  1  1 -1  1 -1 -1 -1 -1 -1 -1]\n",
            "다수결 예측 결과: [1, -1.0, -1.0, -1.0, -1.0, 1, 1, -1.0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
            "real val\n",
            "[ 1  1  1  1  1 -1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cxCbVrCeYeD-",
        "DG48bZ0AevsG",
        "zzcBKutXIEww",
        "as3ta03WIkK2",
        "EIU5cu4p-LbU"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}